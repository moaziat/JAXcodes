{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "BrclF0EvzrD9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Differentiation\n",
        "We introduced **jax.grad** and how it computes the gradients of a given function. Sometimes, we need to define custom gradients for functions that JAX can't differentiate automatically or where we want to improve numerical stability.\n",
        "\n",
        "We are going to introduce **jax.custom_jvp** and **jax.custom_vjp**"
      ],
      "metadata": {
        "id": "daMmFpjSxws7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **custom_jvp** : To define custom [forward-mode ](https://https://en.wikipedia.org/wiki/Automatic_differentiation)differentiation rules"
      ],
      "metadata": {
        "id": "94zEllXKy9JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take the sigmoid function for example\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Its derivative:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "gYh2x2dH0K7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tW_mimf30KJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XziOAyQCxjBr",
        "outputId": "2028c03d-c3d3-41b9-c3fe-3aadb6d99268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "0.25\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from jax import custom_jvp\n",
        "\n",
        "@custom_jvp\n",
        "def sigmoid(x):\n",
        "  return  1 / (1 + jnp.exp(-x))\n",
        "\n",
        "#Now we define its custom gradient\n",
        "\n",
        "@sigmoid.defjvp\n",
        "def sigmoid_jvp(primals, tangents):\n",
        "  x, = primals\n",
        "  x_dot, = tangents\n",
        "  y = sigmoid(x)\n",
        "  return y, y* (1- y) * x_dot\n",
        "\n",
        "x = 0.0\n",
        "print(sigmoid(x)) # sigmoid(0.0) = 1.0 / 1 + 1 = 0.5\n",
        "\n",
        "#manually the output of the gradient should\n",
        "# (d(sigmoid)/dx)[0.0] = 0.5 * (1.0 - 0.5) = 0.25\n",
        "print(jax.grad(sigmoid)(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **custom_vjp**: we introduce this custom gradient when dealing with non-differentiable functions\n",
        "\n",
        "let's suppose the following function: \\begin{align}\n",
        "f(x) = |x|\n",
        "\\end{align}\n",
        "\n",
        "the derivative of this function is not defined at 0. We'll define custom_vjp that handles this situation"
      ],
      "metadata": {
        "id": "AnreLTe73tBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import custom_vjp\n",
        "\n",
        "@custom_vjp\n",
        "def f(x):\n",
        "  return jnp.abs(x)"
      ],
      "metadata": {
        "id": "N8WqWBosznRV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "def f_forward(x):\n",
        "  return f(x), x"
      ],
      "metadata": {
        "id": "fyJNnbjG5MM7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for x > 0:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{df(x)}{dx} &= 1\n",
        "\\end{align}\n",
        "\n",
        "for x < 0:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{df(x)}{dx} &= -1\n",
        "\\end{align}\n",
        "\n",
        "for x = 0:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{df(x)}{dx} &= 0\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "97f_3MmU4leD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#backward pass\n",
        "# df/dx = df/dy * dy/dx : The chain-rule, have look at this https://en.wikipedia.org/wiki/Automatic_differentiation\n",
        "# The code is a bit absurd, please take your time, LOL!!!\n",
        "\n",
        "def f_backward(res, g):\n",
        "  x = res #Residual from the forward pass\n",
        "  return (jnp.where(x > 0, 1.0, jnp.where(x<0, -1.0, 0.0)) * g,)\n"
      ],
      "metadata": {
        "id": "W8PD4hj34lH-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we attach the forward and the backward pass with customized vjp\n",
        "f.defvjp(f_forward, f_backward)"
      ],
      "metadata": {
        "id": "7ARGRfd-4dW9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---- Test\n",
        "\n",
        "#forward pass\n",
        "x = jnp.array([-3.0, -0.8, 1.0, 0.0, 10.0, -22.0])\n",
        "print(f(x)) #Expects to return the abs of each item in x\n",
        "\n",
        "#backward pass\n",
        "grad_f = jax.grad(lambda x : jnp.sum(f(x)))\n",
        "print(grad_f(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbdTOd7x5zWh",
        "outputId": "176771be-0d0f-485d-991e-60ac3043eac2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 3.   0.8  1.   0.  10.  22. ]\n",
            "[-1. -1.  1.  0.  1. -1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRw-F1Dp6Jop"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}